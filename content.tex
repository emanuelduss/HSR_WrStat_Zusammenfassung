%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Kombinatorik (Zählen)}
\subsection{Produktregel: Die Für–jedes–gibt–es–Regel (FJGE)}
Fur jede der $n_1$ Möglichkeiten gibt es eine von der
ersten Position unabhängige Anzahl $n_2$ Möglichkeiten für den Rest.
\[ = n_1 \cdot n_2 \text{ Möglichkeiten } \]

\subsection{Permutationen: Reihenfolge}
Anzahl Anordnungen: Auf wieviele Arten kann man $n$ Objekte anordnen?
\[ = n! \text{ Arten} \]

\subsection{Kombinationen: Auswahl}
Auf wieviele Arten kann man $k$ Objekte aus $n$ auswählen?
\[ = C^n_k=\binom{n}{k} = \frac{n!}{k!(n-k)!} \text{ Arten} \]
Dieser "`Binomialkoeffizient"' lässt sich auf dem Taschenrechner
TI-36XII mit \texttt{n nCr k} (unter \texttt{PRB}), mit dem Voyage 200
mit \texttt{nCr(n,k)}, in Sage mit \texttt{binomial(n,k)} und in 
Octave/Matlab mit \texttt{nchoosek(n,k)} berechnen.

Auf wieviele Arten kann man $k$ Mal eine Auswahl aus $n$ Objekten
treffen?
\[ = n^k \text{ Arten} \]

\subsection{Hypergeometrische Verteilung}

Die Wahrscheinlichkeit, bei einer $n$ Elemente umfassenden Stichprobe
aus einer $N$ Elemente grossen Gesamtheit, in der $M$ ein bestimmtes
Merkmal tragen, deren $m$ mit dem Merkmal zu finden sind:
\[ = \frac{\begin{pmatrix}M\\n\end{pmatrix} \cdot
  \begin{pmatrix}N-M\\n-m\end{pmatrix}}
  {\begin{pmatrix}N\\n\end{pmatrix}} \]

Wie gross ist die Wahrscheinlichkeit $i$ Items einer Art (von welcher es
$a$ gibt) aus der Menge (mit Grösse $m$) mit einer Stichprobengrösse $s$
auszuwählen?
\[ = \frac{\begin{pmatrix}m-a\\s-i\end{pmatrix} \cdot
  \begin{pmatrix}a\\i\end{pmatrix}}
  {\begin{pmatrix}m\\s\end{pmatrix}} \]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Ereignisse und Wahrscheinlichkeit}
\subsection{Wahrscheinlichkeitsexperimente}
\begin{itemize}
  \item Alle möglichen Versuchsausgänge $\Omega$
  \item Elementarereignis $\omega \in \Omega$
  \item Ereignis $A \subset \Omega$
  \item Verschiedene Elementarereignisse $\omega \in A$
  \item Ereignis $A$ tritt ein $\Leftrightarrow \omega \in A$
  \item Wahrscheinlichkeit dass Ereignis $A$ eintritt ist $P(A)$
\end{itemize}

\subsection{Eigenschaften von Wahrscheinlichkeiten}
\begin{itemize}
  \item Laplace-Ereignisse: Alle Elementarereignisse haben die gleiche
    Wahrscheinlichkeit
  \item $P(A) = \frac{|A|}{|\Omega|}$: Wahrscheinlichkeit von
    Elementarereignis $A$
  \item $0 \le P(A) \le 1$: Wahrscheinlichkeit ist immer zwischen $0$
    und $1$
  \item $P(A) < P(B)$: Die Wahrscheinlichkeit für das Ereignis $A$ ist
    kleiner als für $B$
  \item $P(\Omega) = 1$: Das sichere Ereignis tritt immer ein
  \item $P(\emptyset) = 0$: Das unmöglich Ereignis tritt nie ein
\end{itemize}

\subsection{Rechnen mit Wahrscheinlichkeiten}
\begin{itemize}
  \item $P(A \cap B)$: Ereignis $A$ und Ereignis $B$ tritt ein
    \begin{itemize}
      \item Falls unabhängig: $= P(A) \cdot P(B)$
      \item Drei Ereignisse:
        $P(A \cap B \cap C) = P(A) \cdot P(B) \cdot P(B)$
      \item Falls abhängig: Nicht alleine aus $P(A)$ und $P(B)$
        berechenbar!
    \end{itemize}
  \item $P(A \cup B) = P(A) + P(B) - P(A \cap B)$\footnote{Ein-
    Ausschaltformel}: Ereignis $A$ oder Ereignis $B$ tritt ein
    \begin{itemize}
      \item Falls sich $A$ und $B$ nicht überschneiden\footnote{
        Paarweise disjunkt: $A$ und $B$ treffen nicht gleichzeitig ein}:
        $= P(A) + P(B)$
      \item Drei Ereignisse: $P(A \cup B \cup C) \\ = P(A) + P(B) + P(C)
        - P(A \cap B) - P(A \cap C) - P(B \cap C) + P(A \cap B \cap C)$
    \end{itemize}
  \item $P(A \setminus B) = P(A) - P(A \cap B)$: Ereignis $A$ tritt ein, aber ohne
    Ereignis $B$
  \item $P(\bar{A}) = P(\Omega \setminus A) = 1 - P(A)$: Ereignis $A$
    tritt nicht ein
  \begin{itemize}
    \item Bedingte Wahrscheinlichkeit: $P(\bar{A}|B) = 1 - P(A|B)$
  \end{itemize}
\end{itemize}

\subsection{Bedingte Wahrscheinlichkeit}
Wahrscheinlichkeit, dass $A$ eintritt, wenn wir schon wissen, dass $B$
eingetreten ist.\footnote{Somit gilt auch: $P(A) = P(A|\Omega)$}
\[ P(A|B) = \frac{P(A \cap B)}{P(B)} \]
Falls $A$ und $B$ unabhängig sind (z.~B. Behauptung eines Kritikers):
\[ P(A|B) = \frac{P(A) P(B)}{P(B)} = P(A) \]
und
\[P(A) = P(A|B) = P(A|\overline{B}) \text{ und } P(B) = P(B|A)
= P(B|\overline{A})\]


\subsection{Satz der totalen Wahrscheinlichkeit}
Aus Einzelfällen kann man die Gesamtsituation zusammenstellen:
\[ P(A) = \sum_{i=1}^{n}P(A|B_i) \cdot P(B_i) \]

\subsection{Satz von Bayes}
Mit dem Satz von Bayes kann man die Schlussrichtung umkehren
\footnote{Weil $P(A|B) \cdot P(B) = P(A \cap B) = P(B|A) \cdot P(A)$}:
\[ P(A|B) = P(B|A) \cdot \frac{P(A)}{P(B)} \]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Erwartungswert und Varianz}
\subsection{Erwartungswert}
\subsubsection{Begriffe}
\begin{itemize}
  \item Zufallsvariable $X$ ordnet Elementarereignissen $\omega$ Werte
    zu: $X: \Omega \rightarrow \mathbb{R}$
  \item Erwartungswert (Zufallsvariable im Mittel): \\
    $E(X) = \sum_{Werte} \text{Wert } \cdot
    \text{Wahrscheinlichkeit } = \sum_i w_i \cdot P(w_i)$
  \item Empirischer Erwartungswert = arithmetisches Mittel:
    $E(X) = \frac{1}{n} \sum_{i=1}^n x_i$
\end{itemize}
\subsubsection{Spezielle Zufallsvariable}
Charakteristische Funktion von $A$:
\[ \chi_A = \begin{cases}1 & w \in A \\ 0 & sonst \end{cases} \]
\[ E(X) = 0 \cdot P(\bar{A}) + 1 \cdot P(A) = P(A) \]

\subsubsection{Rechnen mit Erwartungswerten}
\begin{itemize}
  \item Multiplikation mit einem Faktor: $E(\lambda X) = \lambda E(X)$
  \item Addition zweier Zufallswerte: $E(X + Y) = E(X) + E(Y)$
  \item Produkt zweier Zufallswerte: $E(X \cdot Y) \ne E(X) \cdot E(Y)$
  \begin{itemize}
    \item Potenzieren (immer abhängig): $E(X^2) \ne E(X)^2$
    \item Wenn die zwei Werte sich nicht beeinflussen
      (unabhängig): $E(X \cdot Y) = E(X) \cdot E(Y)$
  \end{itemize}
  \item Erwartungswert einer Konstante $c$: $E(c) = c$
\end{itemize}

\subsection{Varianz (Streubreite)}
\subsubsection{Begriffe}
\begin{itemize}
  \item Varianz (Mass für die mittlere Abweichung):
    $\operatorname{var}(x) = E((X - E(X))^2) = E(X^2) - E(X)^2 =
    \sum((x-E(x))^2 \cdot p(x))$
  \item Je grösser die Varianz, desto Wahrscheinlicher sind grosse
    Abweichungen vom Mittelwert.
  \item Kovarianz (Verschiebungssatz):
    $\operatorname{cov}(X,Y) = E(XY) - E(X)E(Y)$
  \item Standardabweichung: Abstand ($\pm$) zum Erwartungswert:
    $\sigma = \sqrt{\operatorname{var}(x)}$
\end{itemize}
\subsubsection{Rechenregeln für Varianz}
\begin{itemize}
  \item Multiplikation mit einem Faktor:
    $\operatorname{var}(\lambda X) = \lambda^2 \operatorname{var}(X)$
  \item Addition (nur wenn $X$ und $Y$ unabhängig):
    $\operatorname{var}(X+Y) = \operatorname{var}(X) + \operatorname{var}(Y)$
    \begin{itemize}
      \item Achtung: ($(-x)^2 = +x$): $\operatorname{var}(X-Y) =
      \operatorname{var}(X) + \operatorname{var}(-Y) =
      \operatorname{var}(X) + \operatorname{var}(Y)$
    \end{itemize}
  \item Multiplikation: $\operatorname{var}(X \cdot Y) = \operatorname{var}(X)
    \operatorname{var}(Y) + \operatorname{var}(Y)E(X)^2 + \operatorname{var}(X)E(Y)^2$
  \item Varianz einer Konstante $c$: $\operatorname{var}(c) = 0$
  \item Kovarianz als Verallgemeinerung der Varianz:
    $\operatorname{var}(X) = \operatorname{cov}(X,X)$
\end{itemize}

\subsection{Mittelwert}
\subsubsection{Begriffe}
\begin{itemize}
  \item Erwartungswert der Zufallsvariablen $X$: $\mu = E(X)$
  \item Varianz: Abweichung der Zufallsvariabeln von ihrem
    Erwartungswert: $X -\mu$
  \item Wie häufig überschreitet die Abweichung $\varepsilon$?
  \item Wie wahrscheinlich ist es, dass die Abweichung gross ist?
    $P(|X - \mu| > \varepsilon)$
  \item Faustregel: 10 mal mehr Genauigkeit = 100 mal mehr Arbeit.
\end{itemize}
\subsubsection{Ungleichung von Tschebyscheff}
Genauigkeit des Mittelwertes: Wahrscheinlichkeit, dass Zufallsvariable
$X$ um mehr als $\varepsilon$ vom Erwartungswert abweicht:
\[P(|X-\mu| > \varepsilon) \le \frac{\operatorname{var}(X)}{\varepsilon^2} \]
\subsubsection{Wie gut ist der Mittelwert?}
\begin{itemize}
  \item Mittelwert: $M_n = \frac{X_1 + X_2 + \ldots + X_n}{n}$
  \item Erwartungswert: $E(X_i) = E\left( \frac{X_1 + X_2 + \ldots +
  X_n}{n}\right) =  \frac{E(X_1) + E(X_2) + \ldots + E(X_n)}{n} = \mu$
  \item Varianz: $\operatorname{var}(X_i) = \sigma^2$
  \item Varianz: $\operatorname{var}(M_n) = \frac{1}{n^2} \sum_{i=1}^n
  \operatorname{var}(X_i) = \frac{\sigma^2}{n}$
\end{itemize}
\paragraph{Bernoullis Gesetz der grossen Zahlen}
Die Wahrscheinlichkeit, dass der Mittelwert von $n$ unabhängigen
Zufallsvariabeln mit Mittelwert $\mu$ und Varianz $\sigma^2$ mehr als
$\varepsilon$ von $\mu$ abweicht, ist:
\[P(|X-\mu| > \varepsilon) \le \frac{\operatorname{var}(X)}{\varepsilon^2} \]
Die Abweichung ist unwahrscheinlich, wenn:
\begin{itemize}
  \item Varianz ist klein (genaues Messgerät)
  \item $\varepsilon$ ist gross (man ist toleranter)
  \item $n$ ist gross (viele Messungen)
\end{itemize}

\subsection{Lineare Regression}
Seien $X$ und $Y$ zwei reelle Zufallsvariablen. Die Gerade mit der
Gleichung $y = ax + b$ minimiert die Varianz $\operatorname{var}(aX + b -Y)$
genau dann, wenn
\begin{align*}
  a & = \frac{\operatorname{cov}(X,Y)}{\operatorname{var}(X)} = \frac{E(XY)
        - E(X)E(Y)}{E(X^2) - E(X)^2} \\
  b & = E(Y) - E(X)a
\end{align*}
Die Regression ist umso genauer, je näher der Regressionskoeffizient $r$
bei $\pm 1$ liegt. Zudem hat $r$ immer das gleiche Vorzeichen wie die
Steigung der Regressionsgerade.
\[ r = \frac{\operatorname{cov}(X,Y)}
  {\sqrt{\operatorname{var}(X)\operatorname{var}(Y)}} \]

\begin{itemize}
  \item Ist $r = 0$: Kein linearer Zusammenhang zwischen $x$ und $y$
    (Unabhängig)
  \item Ist $r = 1$: Kein Fehler bei der Approximation
\end{itemize}

Zur Berechnung der Regression eignet sich folgende Tabelle sehr gut,
damit man alle Werte im Überblick hat:

\begin{tabular}{|l|l|l|l|l|l|}
  \hline
  $i$    & $x_i$    & $y_i$    & ${x_i}^2$    & ${y_i}^2$  & $x_i \cdot y_i$ \\
  \hline
  \hline
  1      & $x_1$    & $y_1$    & ${x_1}^2$    & ${y_1}^2$  & $x_i \cdot y_i$ \\
  \vdots & \vdots   & \vdots   & \vdots       & \vdots     & \vdots \\
  \hline
  \hline
  $E$ & $E(x_i)$ & $E(y_i)$ & $E({x_i}^2)$ & $E({y_i}^2)$ & $E(x_i \cdot y_i)$ \\
  \hline
\end{tabular}
\begin{itemize}
  \item Punkt auf X-Achse: $x_i$
  \item Punkt auf Y-Achse: $y_i$
  \item $E(X)$ ist das arithmetische Mittel
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Wahrscheinlichkeitsverteilung}
\subsection{Begriffe}
\begin{itemize}
  \item Verteilungsfunktion: $F(x) = P(X \le x)$
  \item Die Verteilungsfunktion $F$ einer Zufallsvariablen $X$, $F(x) =
    P(X \leq x)$, zeigt die Wahrscheinlichkeit dafür, dass $X$ den Wert
    $x$ nicht überschreitet.
  \item Monoton wachsend: $F(b) - F(a) = P(a < X \leq b) \geq 0$ falls
    $a \leq b$
  \item Ableitung von $F$ (Wahrscheinlichkeitsdichte / Dichtefunktion):
    $F'(x) = \varphi(x)$
  \begin{itemize}
    \item[$\Leftrightarrow$] $F(x) = \int_{-\infty}^{x} \varphi(\tau) d\tau$
    % TODO Verify
  \end{itemize}
  \item Formen der Zufallsvariabeln
  \begin{itemize}
    \item Diskret: Zufallsvariable kann endlich viele Werte annehmen.
      ($F$ ist stückweise konstant (Treppenstufen))
    \item Stetig: Zufallsvariable kann unendlich viele Werte annehmen.
      ($F$ ist ist stetig)
  \end{itemize}
\end{itemize}

\subsection{Eigenschaften der Verteilungsfunktion}
\begin{itemize}
  \item Verteilfunktion zwischen $0$ und $1$: $0 \le F(x) \le 1$
  \item Grenzwerte: $\lim_{x \to -\infty} F(x) = 0$ 
    und $\lim_{x \to \infty} F(x) = 1$
  \item $F$ ist monoton wachsend: $a \le b \Rightarrow F(a) \le F(b)$
\end{itemize}

\subsection{Eigenschaften der Dichtefunktion}
\begin{itemize}
  \item Fläche = 1: $\int_{-\infty}^{\infty} \phi(x) dx = 1$
\end{itemize}

\subsection{Erwartungswert berechnen}
\begin{itemize}
  \item $F$ ist diskret: $E(X) = \sum_i x_i \cdot p(x_i)$ % TODO Verify
  \item $F$ ist stetig: $E(X) = \int_{-\infty}^{\infty}x \cdot \varphi(x)dx$
    und $E(X^2) = \int_{-\infty}^{\infty}x^2 \cdot \varphi(x)dx$
  \item Ist $F$ symmetrisch, gilt $E(X) = 0$
\end{itemize}

\subsection{Rechenregeln für die Verteilungsfunktion}
\begin{itemize}
  \item Umformen um Wahrscheinlichkeit zu berechnen:
    $P(X>n) = 1 - P(X \le n) = 1 - F(n)$
  \item Multiplikation mit einem Faktor $\lambda > 0$: $F_{\lambda X}(x)=
    P(\lambda X \le x) = P(X \le \frac{x}{\lambda}) =
    F_X(\frac{x}{\lambda})$
  \item Addition: $F_{X+a}(x) = P(X+a \le x) = P(X \le x-a) = F_X(x-a)$
  \item Quadrieren: $F_{X^2}(x) = P(X^2 \le x) = P(X \le \sqrt{x}) =
    F_X(\sqrt{x})$
  \item Addition zweier diskreten Zufallszahlen:
    $F_{X+Y}(x) = \sum_x P(X=x)P(Y=z-x)$
\end{itemize}

\subsection{Standardisierung}
Ist $X$ eine Zufallsvariable mit Erwartungswert $\mu$ und Varianz
$\sigma^2$, dann ist
\[ Y = \frac{X-\mu}{\sigma} \]
eine neue Zufallsvariable $Y$ mit $E(Y) = 0$ und $\operatorname{var}(Y) = 0$.
Zwischen den Verteilungsfunktionen $F_X$ und $F_Y$ bestehen die
Beziehungen:
\[ F_Y(y) = F_X(y\sigma+\mu) \qquad \text{ und } \qquad
  F_X(x)=F_Y\left(\frac{x-\mu}{\sigma}\right) \]
Hat die Verteilungsfunktion eine Dichte, dann gilt zudem:
\[ \varphi_Y(y) = \sigma \varphi_X(y\sigma+\mu) \qquad \text{ und }
  \qquad \varphi_X(x) = \frac{1}{\sigma}
  \varphi_Y\left(\frac{x-\mu}\sigma\right) \]

Dasselbe gilt für Wahrscheinlichkeiten:

\[ P(X \leq x_0) = P\left(\frac{X-\mu}{\sigma} \leq \frac{x_0 -
\mu}{\sigma}\right) = P\left(Y \leq \frac{x_0 - \mu}{\sigma}\right) =
F\left(\frac{x_0 - \mu}{\sigma}\right) \]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Katalog von Wahrscheinlichkeitsverteilungen}
\subsection{Stetige Wahrscheinlichkeitsverteilungen}
\subsubsection{Gleichverteilung}
Sei $X$ eine in $[a,b]$ gleichverteilte Zufallsvariable, dann gilt:
\begin{description}
  \item[Anwendung] Verteilung von Zufallszahlen
  \item[Verteilungsfunktion] \[F(x) = \begin{cases} 0 & x < a \\
    \frac{x-a}{b-a} & x \in [a,b] \\ 1 & x > b\end{cases}\]
  \item[Wahrscheinlichkeitsdichte] \[\varphi(x) = \begin{cases} 0 & x < a \\
    \frac{1}{b-a} & x \in [a,b] \\ 0 & x > b\end{cases}\]
  \item[Erwartungswert] \[E(X) = \mu = \frac{a+b}{2}\]
  \item[Varianz] \[\operatorname{var}(X) = \sigma^2 = \frac{(a-b)^2}{12}\]
  \item[Median] \[\operatorname{med}(X) = \frac{a+b}{2}\]
  \item[Wahrscheinlichkeit einer grossen Abweichung] Für $\varepsilon >
  \frac{b-a}{2}$ ist die Wahrscheinlichkeit $P(|X-\mu| > \varepsilon)$
    einer Abweichung vom Erwartungswert $\mu = E(X)$ natürlich 0, aber für
    kleinere $\varepsilon$ ergibt sich
    \[P(|X-\mu| > \varepsilon) = 1 - \frac{2\varepsilon}{b-a}\]
\end{description}
\begin{figure}[!htbp]
  \centering
  \includegraphics[width=10cm]{images/gleichverteilung.pdf}
\end{figure}

\subsubsection{Exponentialverteilung}
\begin{description}
  \item[Anwendung] Prozess ohne Erinnerungsvermögen, radioaktiver
  Zerfall, ermüdungsfreie Bauteile
  \item[Dichtefunktion] \[\varphi(x) = \begin{cases}0 & x < 0 \\ ae^{-ax} &
    x \geq 0\end{cases}\]
  \item[Verteilungsfuntkion] \[F(x) = \begin{cases}0 & x < 0 \\
    1-e^{-ax} & x \geq 0\end{cases}\]
  \item[Erwartungswert] \[E(X) = \mu = \frac{1}{a}\]
  \item[Varianz] \[\operatorname{var}(X) = \sigma^2 = \frac{1}{a^2}\]
  \item[Median] \[\operatorname{med}(X) = \frac{1}{a}\log 2\]
  \item[Wahrscheinlichkeit grosser Abweichung] Für eine
    exponentialverteilte Zufallsvariable mit dem Erwartungswert
    $\frac{1}{a}$ ist die Wahrscheinlichkeit einer Abweichung
    $\varepsilon$ vom Erwartungswert \[P(|X-\frac{1}{a}| > \varepsilon) =
    \begin{cases}e^{-a\varepsilon-1} & \varepsilon > \frac{1}{a} \\
    1 - e^{a\varepsilon-1} + e^{-a\varepsilon-1} & \varepsilon \leq
    \frac{1}{a}\end{cases}\]
  \item[MTBF] Mean Time Between Failure:
    $= \mu = \sigma = \sqrt{\operatorname{var}}$
\end{description}
\begin{figure}[!htbp]
  \centering
  \includegraphics[width=10cm]{images/exponentialverteilung.pdf}
\end{figure}

\subsubsection{Normalverteilung / Gaussverteilung}
\begin{description}
  \item[Anwendung] Messfehler, Summe von vielen voneinander
    unabhängigenn Zufallsvariabeln, Approximation der
    Binomialverteilung, Rauschen
  \item[Dichtefunktion] \[\varphi(x) = \frac{1}{\sigma \sqrt{2\pi}} \cdot
    e^{-\frac{1}{2}{\left(\frac{x-\mu}{\sigma}\right)}^2} =
    \frac{1}{\sigma \sqrt{2\pi}} \cdot e^{-\frac{(x-\mu)^2}{2\sigma^2}}\]
  \item[Verteilungsfunktion] Siehe Tabelle im Anhang.
  \item[Erwartungswert] \[E(X) = \mu\]
  \item[Varianz] \[\operatorname{var}(X) = \sigma^2\]
  \item[Median] \[\operatorname{med}(X) = \mu\]
  \item[Wahrscheinlichkeit] Keine einfache Formel für
    $P(|X - E(X)| > \varepsilon)$
\end{description}
\begin{figure}[!htbp]
  \centering
  \includegraphics[width=10cm]{images/normalverteilung.pdf}
\end{figure}

% \subsection{2-Verteilung}

\subsubsection{Potenzverteilung / Pareto-Verteilung}
\begin{description}
  \item[Anwendung] Häufkeitsverteilung für skaleninvariante Prozesse,
    Einkommensverteilung Grösse und Häufigkeit von Mondkratern,
    Verkaufszahlen von Büchern, Einwohnerzahlen von Städten
  \item[Dichtefunktion] \[ \varphi(X) = \begin{cases} Cx^{-\alpha} & x \geq
    x_{min} \\ 0 & x < x_{min}\end{cases} \] Typische Werte für $\alpha$:
    $1 \leq \alpha \leq 3$
  \item[Verteilungsfunktion]
  \item[Varianz]
  \item[Median]
  \item[Wahrscheinlichkeit]
\end{description}

\subsection{Diskrete Wahrscheinlichkeitsverteilungen}
\subsubsection{Gleichverteilung}
\begin{description}
  % TODO Andere Werte
  \item[Erwartungswert] \[ E(X) = \frac{n+1}{2} \]
  \item[Varianz] \[ \operatorname{var}(X) = \frac{n^2-1}{12}\]
\end{description}

\subsubsection{Binomialverteilung}
\begin{description}
  \item[Anwendungen] Anzahl Einheiten eines Bernoulli-Ereignisses bei
    $n$ Wiederholungen ($2$ mögliche Versuchsausgänge).
  \item[Wahrscheinlichkeit] Eine Zufallsvariable mit diskreten Werten
    $k \in \{0, \dots, n\}$ heisst binomialverteilt zum Parameter $p$,
    (Wahrscheinlichkeit) wenn folgendes die Wahrscheinlichkeit des
    Wertes $k$ ist. Wahrscheinlichkeit, dass $X = 1$ k-mal eingetreten
    ist:
    \[P(X = k) = \binom{n}{k} p^k(1-p)^{n-k}\]
    \begin{itemize}
      \item $n$ = Anzahl Versuche
      \item $p$ = Wahrscheinlichkeit einer Zufallsvariable
      \item $k$ = Anzahl eintreffende Ereignisse
    \end{itemize}
  \item[Verteilungsfunktion] \[F(k) = \sum_{i=0}^k \binom{n}{i} p^i(1-p)^{n-i}\]
  \item[Erwartungswert] \[E(X) = np\]
  \item[Varianz] \[\operatorname{var}(X) = np(1-p)\]
    Maximale Varianz wird erreicht bei $p = \frac{1}{2}$
  \item[Schlecht berechenbar] Ist $n$ und $p$ gross, dauert das
  berechnen der Wahrscheinlichkeit sehr lange. Dann kann man davon
  ausgehen, dass die Zufallsvariable $X$ annähernd normalverteilt ist.
\end{description}

%\subsubsection{Hypergeometrische Verteilung}

\subsubsection{Poissonverteilung}
\begin{description}
  \item[Anwendung] Anzahl Ereignisse in einem Zeitintervall, wenn die
    Zeitabstände exponentiell verteilt sind. Approximation der
    Binomialverteilung für seltene Ereignisse, die mit Rate $\lambda$
    eintreten.
  \item[Verteilungsfunktion] Sind $(X_i)1 \leq i \leq k$
    exponentialverteilte, unabhängige Zufallsvariablen, dann gilt für die
    Summe:
  \[F_{X_i+\dots+x_k}(x) = \begin{cases}1 - e^{-ax} \sum_{i=0}^{k-1}
  \frac{(ax)^i}{i!} & x \geq 0 \\ 0 & x < 0\end{cases}\]
  \item[Wahrscheinlichkeitsdichte]
  \[\varphi_{X_i+\dots+x_k}(x) = \begin{cases}a^k \frac{k^{k-1}}{(k-1)!}
  e^{-ax} & x \geq 0 \\ 0 & x < 0\end{cases}\]
  \item[Wahrscheinlichkeit] Beschreibt für $\lambda = ax$ die
    Wahrscheinlichkeit, dass in einem Zeitintervall $[0, x]$ genau $k$
    Ereignisse eintreten, wenn die Zeit zwischen den Ereignissen
    exponentialverteilt ist mit der Dichte $ae^{-ax}$ ($\lambda$ =
    Häufigheit oder Rate, in der Ereignisse auftreten).
    \[P_\lambda(k) = e^{-\lambda} \cdot \frac{\lambda^k}{k!}\]
  \item[Erwartungswert] \[E(X) = \lambda\]
  \item[Varianz] \[\operatorname{var}(X) = \lambda\]
\end{description}

\subsubsection{Poissonverteilung als Approximation für die
Binomialverteilung}
\begin{itemize}
  \item Zufallsvariable $X$ ist binomialverteilt
  \item Ist $n$ bei der Binomialverteilung gross, ist diese nicht
    durchführbar (zulange Rechenzeit).
  \item Ist $X$ annähernd normalverteilt, gilt:
    $E(X) = \mu = np$ und $\operatorname{var}(X) = \sigma^2 = np(1-p)$
\end{itemize}
Verteilfunktion:
\[ P(X \le x) = P\left(\frac{X - \mu}{\sigma} \le
  \frac{x-\mu}{\sigma}\right) = F\left(\frac{x-np}{\sqrt{np(1-p)}}\right)\]
Poissonverteilung zum Parameter $\lambda$:
\[ P_{\lambda}(k) = e^{-\lambda} \frac{\lambda^k}{k!}\]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Schätzen}
\subsection{Begriffe}
\begin{description}
  \item[Schätzer] Formel $\hat{\vartheta}(x_1, x_2, \dots, x_n)$ für
  einen Parameter wie $\mu, \sigma^2, n, p, \lambda$.
  \item Stichprobe der Zufallsvariabeln $X$, wenn die Zufallsvariabeln
    $X_i$ unabhängig und identisch zu $X$ verteilt sind.
\end{description}
\subsection{Qualitätskriterien für Schätzer}
\begin{description}
  \item[Konsistente Schätzer] Unendliche Stichproben ergeben den exakten
  Parameterwert (fast immer erfüllt).
    \[ \lim_{n \rightarrow \infty} \vartheta(X_1, \dots, X_n) \]
  \item[Erwartungstreue Schätzer] Im Mittel richtig, auch bei kleinen n.
    \[ E(\hat{\vartheta}(x_1, \dots, x_n)) = \vartheta \]
\end{description}
\subsection{Schätzer finden}
Likelihood-Funktion
\[ L(x_1, \dots, x_n) = \varphi(x_1) \cdot \ldots \cdot \varphi(x_n)\]

\subsection{Bekannte Schätzer}
\begin{description}
  \item[Stichprobenmittelwert] Der Schätzer für den
    Stichprobenmittelwert der Stichproben $X_1, \dots, X_n$ heisst:
    \[ \hat{X} = \hat{\mu}(X_1, \dots, X_n) = \frac{X_1 + \ldots +
    X_n}{n} = \frac{1}{n} \sum_{i=1}^{n} X_i \]
  \item[Stichprobenvarianz] Der Schätzer für die Stichprobenvarianz der
    Stichproben $X_1, \dots, X_n$ heisst:
    \[ S^2 = \hat{\sigma^2}(x_1, \dots, x_n) = \frac{1}{n-1}
    \sum_{i=1}^{n}(X_i - \overline{X})^2 = \frac{n}{n-1}\left( \frac{1}{n}
    \sum {x_i}^2 - \left(\frac{1}{n} \sum x_i\right)^2\right)\]
  \item[Länge eines Intervalls] Ist $X_i$ eine Stichprobe einer auf dem
    Intervall $[0, \vartheta]$ gleichverteilten Zufallsvariable $X$, dann ist
    folgende Formel ein erwartungstreuer Schätzer für die Intervallänge
    $\vartheta$:
    \[ \hat{\vartheta} = \frac{n+1}{n} \operatorname{max}(X_1, \dots, X_n)\]
  \item[Parameter $\lambda$ einer Poissonverteilung] Der Schätzer für
    den Parameter $\lambda$ einer Poissonverteilung ist:
    \[ \lambda(X_1, \dots, X_n) = \frac{1}{n} \sum_{i=1}^{n} X_i \]
  \item[Parameter $p$ einer Binomialverteilung] Der Schätzer für den
    Parameter $p$ einer Binomialkoeffizient mit den bekannten Parametern
    $m$ ist:
    \[ p(X_1, \dots, X_n) = \frac{1}{nm} \sum_{i=1}^{n} X_i \]
\end{description}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Hypothesentests - Wie wissen wir, das etwas wahr ist?}
\subsection{Aussagen}
\begin{itemize}
  \item Aussagen müssen falsifizierbar sein
  \item Positive Aussagen brauchen einen Beweis
  \item Negative Aussagen können nicht bewiesen werden
  \item Ausgangsposition ist ''wir akzeptieren nichts``
  \item Aussagen werden provisorisch akzeptier
\end{itemize}

\subsection{Begriffe}
\begin{itemize}
  \item Nullhypothese: Ausgangsposition
  \item Hypothese: Positive falsifizierbare Aussage
  \begin{itemize}
    \item Falsifizierbar: Messgrösse, die entweder wahr oder falsch ist
  \end{itemize}
\end{itemize}

\subsection{Testen der Wahrscheinlichkeit eines Ereignisses (Diskret)}
\subsubsection{$\chi^2$-Test}
\begin{itemize}
  \item Nullhypothese: ''Gleichviele orange wie rote Bonbons``
  \item Hypothese: ``Mehr orange als rote Bonbons``
\end{itemize}
Diskrepanz $D$ (Abweichung von der Nullhypothese) berechnen:
\[ D = \sum \frac{(n_i - np_i)^2}{np_i} \]

Mit Hilfe dieser Tabelle:

\begin{tabular}{|l|l|l|l|l|l|}
  \hline
  Mögliche Werte & Wahrscheinlichkeit ($p_i$) & Anzahl Ereignisse ($n_i$) & $np_i$ & $n_i - np_i$ & $\frac{(n_i - np_i)^2}{np_i}$ \\
  \hline
  \hline
  Gelb    &   0.25    &  11       &   15.75    &  4.75    &  1.432 \\
  Orange  &   0.25    &  23       &   15.75    &  7.25    &  3.34  \\
  Rot     &   0.25    &  10       &   15.75    &  5.75    &  2.09  \\
  Grün    &   0.25    &  19       &   15.75    &  3.25    &  0.67  \\
  \hline
  \hline
  Total   &   1       &  63 = $n$ &   15.75    &          & $D$    \\
  \hline
\end{tabular}


Die Diskrepanz $D$ ist für genügend grosse $n$ annhährend $\chi^2$
verteilt mit $k$ Freiheitsgraden ($k = $ Anzahl Ausgänge $-1$).

Ist $D < D_{\text{Krit}}$, gibt es keinen Grund an der Nullhypothese zu
zweifeln.

Einschränkungen:
\begin{itemize}
  \item Bei wenigen Testdaten nicht anwendbar
  \item Ist $p_i$ klein, sind sehr viele Beobachtungen nötig
  \item Faustregel: Jedes Intervall sollte mindestens 5 Einträge haben:
    $n_i \geq 5$
\end{itemize}

\subsection{Testen der Wahrscheinlichkeit eines Ereignisses (Stetig)}
\subsubsection{Kolmogoroff-Smirnov-Test}
\begin{itemize}
  \item Nullhypothese: Daten sind gemäss der Verteilungsfunktion $F$
    verteilt.
  \item Hypothese: ``Das ist nicht so.''
  \item Idee: Unterschied zwischen der Verteilfunktion $F$
    und der gemessenen Verteilfunktion $F_{\text{Empirisch}}$ messen.
  \item Gesucht: Grösste Abweichung zwischen $F$ und $F_{\text{Empirisch}}$
\end{itemize}

\[ F_{\text{Empirisch}}(x) = \frac{|\{x_i | X_i \leq x\}|}{n} \]

Berechnung mit Tabelle (Die $x_j$ Werte müssen aufsteigend sortiert
sein!):

\begin{tabular}{|l|l|l|l|l|}
  \hline
  $j$        & $x_j$     & $F(x_j)$    & $\frac{j}{n}-F(x_i)$  & $F(x_j) - \frac{j-1}{n}$ \\
  \hline
  \hline
  1         &           &           &            &          \\
  2         &           &           &            &          \\
  $\vdots$  &           &           &            &          \\
  n         &           &           &            &          \\
  \hline
  \hline
          &           &  & $\operatorname{max}(...)$ & $\operatorname{max}(...)$ \\
  \hline
\end{tabular}


\[ K_n^+ = \sqrt{n} \cdot \max_{-\infty<x<\infty} \left(\frac{j}{n} - F(x_j)\right)
, \qquad
K_n^- = \sqrt{n} \cdot \max_{-\infty<x<\infty} \left(F(x_j) - \frac{j-1}{n} \right) \]

Der kritische Wert $K_{\text{krit}}$ kann man aus der Tabelle auslesen.
Ist $k_{\text{krit}} > K^\pm$, ist die Nullhypothese falsch.

Wie ist die Verteilung von $F(X)$?

\[ F_{F(X)}(X) = P(F(X) \leq x) = P(X \leq F^{-1}(x)) = F_X(F^{-1}(x)) =
x \]

\subsubsection{t-Test}
\begin{itemize}
  \item Nullhypothese: Zwischen zwei Messreihen gibt es kein Unterschied
    (Messreihen sind normalverteilt).
  \item Hypothese: Es gibt einen Unterschied.
  \item Nützlich, wenn beide Messreihen grosse Unterschiede haben
    (Varianz ist gross)
  \item Anzahl Messungen für $X$: $n$ und Anzahl Messungen für $Y$: $m$
  \item Zwei Mittelwerte der Messreihe $X$ unx $Y$:
    $\overline{X} = \mu_x$ und $\overline{Y} = \mu_y$
  \item Varianz von $X$: ${S_X}^2 = \operatorname{var}(X)$ und Varianz
    von $Y$: $d {S_Y}^2 = \operatorname{var}(Y)$
\end{itemize}

\[ T = \frac{\overline{X} - \overline{Y}}{\sqrt{(n-1) {S_X}^2 + (m-1)
{S_Y}^2}} \cdot \sqrt{\frac{nm(n+m-2)}{n+m}}\]

$T$ ist t-Verteilt (vgl. Tabelle). Man legt ein $\alpha$ fest und berechnet den
kritischen Wert. Ist $t > t_{krit}$, verwirft man die Nullhypothese.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Filter}
\subsection{Kalman-Filter}
\begin{itemize}
  \item Anwendung: GPS-Empfänger, Mondflug, Scheitelbestimmung einer
    Raketenbahn
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\clearpage
\thispagestyle{empty}
\begin{appendices}
\pagenumbering{roman}
\section{Summenformeln}
\[ \begin{aligned}
	\sum_{i=1}^{n}i     & = \frac{n(n+1)}{2} \\
	\sum_{i=1}^{n}i^2   & = \frac{n(n+1)(2n+1)}{6} \\
	\sum_{i=1}^{n}i^3   & = \left(\frac{n(n+1)}{2}\right)^2 \\
\end{aligned} \]
\section{Tabellen}
Die Tabellen wurden aus dem Skript der Vorlesung von Herr Müller
übernommen und ergänzt.
\input{tabellen.tex}
\end{appendices}
